[core]
# Airflow configuration for financial planning data platform
dags_folder = /opt/airflow/dags
base_log_folder = /opt/airflow/logs
remote_logging = True
logging_config_class = log_config.DEFAULT_LOGGING_CONFIG
executor = CeleryExecutor
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
dags_are_paused_at_creation = False
max_active_runs_per_dag = 1
load_examples = False
plugins_folder = /opt/airflow/plugins
donot_pickle = True
dagbag_import_timeout = 30
dagbag_import_error_tracebacks = True
dagbag_import_error_traceback_depth = 2
dag_file_processor_timeout = 50
task_runner = StandardTaskRunner
default_task_retries = 2
parallelism = 32
dag_concurrency = 16
worker_concurrency = 16
max_active_tasks_per_dag = 16
killed_task_cleanup_time = 60
dag_discovery_safe_mode = True
default_pool_task_slot_count = 128
max_db_retries = 3

[scheduler]
dag_dir_list_interval = 300
child_process_log_directory = /opt/airflow/logs/scheduler
scheduler_heartbeat_sec = 5
num_runs = -1
scheduler_idle_sleep_time = 1
min_file_process_interval = 0
dag_file_processor_timeout = 50
print_stats_interval = 30
pool_metrics_interval = 5.0
scheduler_health_check_threshold = 30
orphaned_tasks_check_interval = 300.0
child_process_log_directory = /opt/airflow/logs/scheduler
scheduler_zombie_task_threshold = 300
catchup_by_default = False
max_tis_per_query = 512

[celery]
worker_concurrency = 16
worker_log_server_port = 8793
broker_url = redis://redis:6379/0
result_backend = db+postgresql://airflow:airflow@postgres:5432/airflow
flower_host = 0.0.0.0
flower_port = 5555
default_queue = default

[webserver]
base_url = http://localhost:8080
web_server_host = 0.0.0.0
web_server_port = 8080
web_server_ssl_cert = 
web_server_ssl_key = 
workers = 4
worker_timeout = 120
worker_refresh_batch_size = 1
worker_refresh_interval = 30
secret_key = temporary_key
workers = 4
worker_timeout = 120
access_logfile = -
error_logfile = -
expose_config = True
authenticate = True
auth_backend = airflow.contrib.auth.backends.password_auth

[smtp]
smtp_host = localhost
smtp_starttls = True
smtp_ssl = False
smtp_user = 
smtp_password = 
smtp_port = 587
smtp_mail_from = airflow@financialplanning.com

[kubernetes]
namespace = airflow
airflow_configmap = airflow-config
worker_container_repository = apache/airflow
worker_container_tag = 2.7.1
worker_container_image_pull_policy = IfNotPresent
delete_worker_pods = True
delete_worker_pods_on_success = False
delete_worker_pods_on_failure = False
git_repo = 
git_branch = 
git_subpath = 
git_user = 
git_password = 
git_sync_root = /git
git_sync_dest = repo
dags_volume_claim = airflow-dags
logs_volume_claim = airflow-logs
in_cluster = True
cluster_context = 
config_file = 
node_selector_label_key = 
node_selector_label_value = 

[elasticsearch]
host = 
log_id_template = {dag_id}_{task_id}_{execution_date}_{try_number}
end_of_log_mark = end_of_log
frontend = 
write_stdout = False
json_format = False
json_fields = asctime, filename, funcName, levelname, lineno, message, thread, threadName

[api]
auth_backend = airflow.api.auth.backend.basic_auth
enable_experimental_api = False

[lineage]
backend = 

[atlas]
sasl_enabled = False
host = 
port = 21000
username = 
password = 

[hive]
default_hive_mapred_queue = 

[ldap]
uri = 
user_filter = objectClass=*
user_name_attr = uid
group_member_attr = memberOf
superuser_filter = 
data_profiler_filter = 
bind_user = cn=Manager,dc=example,dc=com
bind_password = insecure
basedn = dc=example,dc=com
cacert = /etc/ca/ldap_ca.crt
search_scope = LEVEL

[kerberos]
ccache = /tmp/airflow_krb5_ccache
principal = airflow
reinit_frequency = 3600
kinit_path = kinit
keytab = airflow.keytab

[github_enterprise]
api_rev = v3

[admin]
hide_sensitive_variable_fields = True