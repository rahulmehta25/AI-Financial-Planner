# Cluster Autoscaler configuration for Financial Planning Application
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    app: cluster-autoscaler
    app.kubernetes.io/name: cluster-autoscaler
    app.kubernetes.io/component: autoscaling
    app.kubernetes.io/version: "1.28.2"
  annotations:
    description: "Cluster autoscaler for automatic node scaling"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8085"
        prometheus.io/path: "/metrics"
        cluster-autoscaler.kubernetes.io/safe-to-evict: "false"
    spec:
      serviceAccountName: cluster-autoscaler
      priorityClassName: system-cluster-critical
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
        fsGroup: 65534
        seccompProfile:
          type: RuntimeDefault
      containers:
      - image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.28.2
        name: cluster-autoscaler
        resources:
          limits:
            cpu: 100m
            memory: 600Mi
          requests:
            cpu: 100m
            memory: 600Mi
        command:
        - ./cluster-autoscaler
        - --v=4
        - --stderrthreshold=info
        - --cloud-provider=aws
        - --skip-nodes-with-local-storage=false
        - --expander=least-waste
        - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/financial-planning-cluster
        - --balance-similar-node-groups
        - --skip-nodes-with-system-pods=false
        - --scale-down-enabled=true
        - --scale-down-delay-after-add=10m
        - --scale-down-unneeded-time=10m
        - --scale-down-delay-after-delete=10s
        - --scale-down-delay-after-failure=3m
        - --scale-down-utilization-threshold=0.5
        - --scale-down-non-empty-candidates-count=30
        - --scale-down-candidates-pool-ratio=0.1
        - --scale-down-candidates-pool-min-count=50
        - --max-node-provision-time=15m
        - --max-empty-bulk-delete=10
        - --max-nodes-total=100
        - --cores-total=0:16000
        - --memory-total=0:16000000
        - --node-autoprovisioning-max-node-provision-time=15m
        - --ok-total-unready-count=3
        - --max-autoprovisioned-node-group-count=15
        - --enable-dynamic-provisioning=true
        - --logtostderr=true
        - --write-status-configmap=true
        - --leader-elect=true
        - --leader-elect-resource-lock=leases
        - --feature-gates=InPlacePodVerticalScaling=true
        env:
        - name: AWS_REGION
          value: us-east-1
        - name: AWS_DEFAULT_REGION
          value: us-east-1
        volumeMounts:
        - name: ssl-certs
          mountPath: /etc/ssl/certs/ca-certificates.crt
          readOnly: true
        imagePullPolicy: Always
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
      volumes:
      - name: ssl-certs
        hostPath:
          path: "/etc/ssl/certs/ca-bundle.crt"
      nodeSelector:
        kubernetes.io/arch: amd64
        kubernetes.io/os: linux
        node-type: system
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/control-plane
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
      - effect: NoSchedule
        key: CriticalAddonsOnly
        operator: Exists

---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
  name: cluster-autoscaler
  namespace: kube-system
  annotations:
    eks.amazonaws.com/role-arn: "arn:aws:iam::123456789012:role/ClusterAutoscalerRole"

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
  - apiGroups: [""]
    resources: ["events", "endpoints"]
    verbs: ["create", "patch"]
  - apiGroups: [""]
    resources: ["pods/eviction"]
    verbs: ["create"]
  - apiGroups: [""]
    resources: ["pods/status"]
    verbs: ["update"]
  - apiGroups: [""]
    resources: ["endpoints"]
    resourceNames: ["cluster-autoscaler"]
    verbs: ["get", "update"]
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["watch", "list", "get", "update"]
  - apiGroups: [""]
    resources:
      - "namespaces"
      - "pods"
      - "services"
      - "replicationcontrollers"
      - "persistentvolumeclaims"
      - "persistentvolumes"
    verbs: ["watch", "list", "get"]
  - apiGroups: ["extensions"]
    resources: ["replicasets", "daemonsets"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["policy"]
    resources: ["poddisruptionbudgets"]
    verbs: ["watch", "list"]
  - apiGroups: ["apps"]
    resources: ["statefulsets", "replicasets", "daemonsets"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses", "csinodes", "csidrivers", "csistoragecapacities"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["batch", "extensions"]
    resources: ["jobs"]
    verbs: ["get", "list", "watch", "patch"]
  - apiGroups: ["coordination.k8s.io"]
    resources: ["leases"]
    verbs: ["create"]
  - apiGroups: ["coordination.k8s.io"]
    resourceNames: ["cluster-autoscaler"]
    resources: ["leases"]
    verbs: ["get", "update"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["create","list","watch"]
  - apiGroups: [""]
    resources: ["configmaps"]
    resourceNames: ["cluster-autoscaler-status", "cluster-autoscaler-priority-expander"]
    verbs: ["delete", "get", "update", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
# Priority Class for Cluster Autoscaler
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: system-cluster-critical
value: 2000000000
globalDefault: false
description: "Used for system critical pods that must run on the cluster."

---
# Service Monitor for Prometheus
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    app.kubernetes.io/name: cluster-autoscaler
    app.kubernetes.io/component: monitoring
spec:
  selector:
    matchLabels:
      app: cluster-autoscaler
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics

---
apiVersion: v1
kind: Service
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    app: cluster-autoscaler
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8085"
    prometheus.io/path: "/metrics"
spec:
  selector:
    app: cluster-autoscaler
  ports:
  - name: metrics
    port: 8085
    targetPort: 8085
    protocol: TCP

---
# ConfigMap for Cluster Autoscaler Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-autoscaler-status
  namespace: kube-system
  labels:
    app.kubernetes.io/name: cluster-autoscaler
    app.kubernetes.io/component: config
data:
  # Cluster Autoscaler configuration parameters
  nodes.max: "100"
  nodes.min: "3"
  scale-down-enabled: "true"
  scale-down-delay-after-add: "10m"
  scale-down-unneeded-time: "10m"
  scale-down-utilization-threshold: "0.5"
  skip-nodes-with-local-storage: "false"
  skip-nodes-with-system-pods: "false"
  expander: "least-waste"

---
# Node Group Configuration for different workload types
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-autoscaler-node-groups
  namespace: kube-system
  labels:
    app.kubernetes.io/name: cluster-autoscaler
    app.kubernetes.io/component: node-groups
data:
  general-compute.yaml: |
    # General compute nodes for standard workloads
    nodeGroup:
      name: general-compute
      minSize: 2
      maxSize: 20
      desiredCapacity: 3
      instanceTypes:
        - m5.large
        - m5.xlarge
        - m5.2xlarge
      labels:
        node-type: general-compute
        workload-type: standard
      taints: []
      
  compute-optimized.yaml: |
    # Compute optimized nodes for API services
    nodeGroup:
      name: compute-optimized
      minSize: 3
      maxSize: 30
      desiredCapacity: 5
      instanceTypes:
        - c5.large
        - c5.xlarge
        - c5.2xlarge
        - c5.4xlarge
      labels:
        node-type: compute-optimized
        workload-type: api
      taints:
        - key: financial-planning.com/api
          value: "true"
          effect: NoSchedule
          
  memory-optimized.yaml: |
    # Memory optimized nodes for databases
    nodeGroup:
      name: memory-optimized
      minSize: 1
      maxSize: 10
      desiredCapacity: 2
      instanceTypes:
        - r5.large
        - r5.xlarge
        - r5.2xlarge
        - r5.4xlarge
      labels:
        node-type: memory-optimized
        workload-type: database
      taints:
        - key: financial-planning.com/database
          value: "true"
          effect: NoSchedule
          
  gpu-nodes.yaml: |
    # GPU nodes for ML workloads
    nodeGroup:
      name: gpu-nodes
      minSize: 0
      maxSize: 5
      desiredCapacity: 1
      instanceTypes:
        - p3.2xlarge
        - p3.8xlarge
        - g4dn.xlarge
        - g4dn.2xlarge
      labels:
        node-type: gpu
        workload-type: ml
        accelerator: nvidia-tesla-v100
      taints:
        - key: nvidia.com/gpu
          value: "true"
          effect: NoSchedule
        - key: gpu-workload
          value: "true"
          effect: NoSchedule